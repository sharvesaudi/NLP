{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Vanakam** 🤝"
      ],
      "metadata": {
        "id": "a1m_uzGbeKs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tDerive a top-down, depth-first, left-to-right parse tree for the given sentence:\n",
        "“The angry bear chased the frightened little squirrel”\n",
        "Use the following grammar rules to create the parse tree:\n",
        "S → NP VP\n",
        "NP → Det Nom\n",
        "VP → V NP\n",
        "Nom →Adj Nom | N\n",
        "Det → the\n",
        "Adj→ little | angry | frightened\n",
        "N → squirrel | bear\n",
        "V → chased\n"
      ],
      "metadata": {
        "id": "4bpUPgKeqUEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> Det Nom\n",
        "VP -> V NP\n",
        "Nom -> Adj Nom | N\n",
        "Det -> 'the'\n",
        "Adj -> 'little' | 'angry' | 'frightened'\n",
        "N -> 'squirrel' | 'bear'\n",
        "V -> 'chased' \"\"\")\n",
        "sentence = 'the angry bear chased the frightened little squirrel'.split() \n",
        "def parse(sent):\n",
        "  parser = nltk.ChartParser(grammar) \n",
        "  for tree in parser.parse(sent):\n",
        "    return tree\n",
        "\n",
        "print(parse(sentence)) \n",
        "parse(sentence).pretty_print()"
      ],
      "metadata": {
        "id": "mlVyzYriqTSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.\tCreate the grammar rules for the following sentence and implement the same to generate the following parse tree."
      ],
      "metadata": {
        "id": "xeS8WhULIjRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> Det Nom | N\n",
        "VP -> V NP\n",
        "Nom ->  N\n",
        "Det -> 'a'\n",
        "Adj -> 'little' | 'angry' | 'frightened'\n",
        "N -> 'restaurant' | 'dosa'\n",
        "V -> 'serves' \"\"\")\n",
        "sentence = 'a restaurant serves dosa'.split() \n",
        "def parse(sent):\n",
        "  parser = nltk.ChartParser(grammar) \n",
        "  for tree in parser.parse(sent):\n",
        "    return tree \n",
        "  \n",
        "print(parse(sentence)) \n",
        "parse(sentence).pretty_print()"
      ],
      "metadata": {
        "id": "_60qzViDImSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tImplement the Minimum edit distance algorithm and find the minimum number of operations required to convert string1 to string2.\n",
        "\n",
        "str1 = 'Saturday' \n",
        "\n",
        "str2 = 'Sunday'"
      ],
      "metadata": {
        "id": "AXTcB3-ZM4_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_distance(str1, str2, a, b):\n",
        "    string_matrix = [[0 for i in range(b+1)] for i in range(a+1)]\n",
        "\n",
        "    for i in range(a+1):\n",
        "        for j in range(b+1):\n",
        "\n",
        "            if i == 0:\n",
        "                string_matrix[i][j] = j \n",
        "            elif j == 0:\n",
        "                string_matrix[i][j] = i   \n",
        "            elif str1[i-1] == str2[j-1]:\n",
        "                string_matrix[i][j] = string_matrix[i-1][j-1]  \n",
        "            else:\n",
        "                string_matrix[i][j] = 1 + min(string_matrix[i][j-1],      \n",
        "                                       string_matrix[i-1][j],      \n",
        "                                       string_matrix[i-1][j-1])    \n",
        "\n",
        "    return string_matrix[a][b]\n",
        "\n",
        "str1 = 'Saturday'\n",
        "str2 = 'Sunday'\n",
        "\n",
        "print('No. of Operations required :',edit_distance(str1, str2, len(str1), len(str2)))\n"
      ],
      "metadata": {
        "id": "_okortxvM5cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tGiven the corpus below, find the most probable next word following the sequences "
      ],
      "metadata": {
        "id": "5MpRQlbYNAym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#N gram\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import reuters\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "trigram_freq = defaultdict(Counter)\n",
        "trigram_prob = defaultdict(Counter)\n",
        "def process_sentences(sentences):\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # Create trigrams for the sentence\n",
        "        trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "        # Update trigram frequencies\n",
        "        for t1, t2, t3 in trigrams:\n",
        "            trigram_freq[(t1, t2)][t3] += 1\n",
        "\n",
        "    # Update trigram probabilities\n",
        "    for (t1, t2), t3_freq in trigram_freq.items():\n",
        "        total_count = sum(t3_freq.values())\n",
        "        for t3, count in t3_freq.items():\n",
        "            trigram_prob[(t1, t2)][t3] = count / total_count\n",
        "\n",
        "#N gram\n",
        "def find_next_word(keyword1, keyword2):\n",
        "    next_word = None\n",
        "    max_prob = 0\n",
        "\n",
        "    for (t1, t2), t3_prob in trigram_prob.items():\n",
        "        if t1 == keyword1 and t2 == keyword2:\n",
        "            for t3, prob in t3_prob.items():\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    next_word = t3\n",
        "\n",
        "    return next_word\n",
        "\n",
        "\n",
        "input_sentences = [\n",
        "    \"<S> I am Henry </S>\",\n",
        "\"<S> I like college </S>\",\n",
        "\"<S> Do Henry like college </S>\"\n",
        "\"<S> Henry I am </S>\",\n",
        "\"<S> Do I like Henry </S>\",\n",
        "\"<S> Do I like college </S>\",\n",
        "\"<S> I do like Henry </S>\"\n",
        "]\n",
        "\n",
        "process_sentences(input_sentences)\n",
        "\n",
        "\n",
        "keyword1 = \"I\"\n",
        "keyword2 = \"like\"\n",
        "next_word = find_next_word(keyword1, keyword2)\n",
        "print(f\"The most probable next word after '{keyword1} {keyword2}' is '{next_word}'\")\n"
      ],
      "metadata": {
        "id": "oIBB8W2MNER4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ce1222-6315-4f88-b8e2-95471f3bf75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most probable next word after 'I like' is 'college'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tImplement Stemming and Lemmatization algorithm and find the output for the following words.\n",
        "\n",
        "-\tProgramming\n",
        "-\tLoving\n",
        "-\tLovely\n",
        "-\tKind\n"
      ],
      "metadata": {
        "id": "CJ90C7zNnuuW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlwNliA6ntoD"
      },
      "outputs": [],
      "source": [
        "#stemming\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ans = PorterStemmer()\n",
        "text = \"Programming Loving Lovely Kind\"\n",
        "token = nltk.word_tokenize(text)\n",
        "for i in token:\n",
        "  print(\"Stemming for {} is {}\".format(i, ans.stem(i)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "ans = WordNetLemmatizer()\n",
        "text = \"Programming Loving Lovely Kind\"\n",
        "token = nltk.word_tokenize(text)\n",
        "for i in token:\n",
        "  print(\"Lemma for {} is {}\".format(i,ans.lemmatize(i)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fCuVZ1VoxPS",
        "outputId": "4d32d06e-efb5-4278-a13a-7c46740034fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for Programming is Programming\n",
            "Lemma for Loving is Loving\n",
            "Lemma for Lovely is Lovely\n",
            "Lemma for Kind is Kind\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\tImplement and do Parts of speech tagging for the following sentences. \n",
        "-\tI need a flight from Atlanta.\n",
        "-\tEverything to permit us.\n",
        "-\tI would like to address the public on this issue.\n",
        "-\tWe need your shipping address.\n"
      ],
      "metadata": {
        "id": "R768yacXJEcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parts of speech\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "sentences = [\n",
        " \"I need a flight from Atlanta.\",\n",
        " \"Everything to permit us.\",\n",
        " \"I would like to address the public on this issue.\"\n",
        "\"We need your shipping address.\"\n",
        "]\n",
        "for sentence in sentences:\n",
        " words = nltk.word_tokenize(sentence)\n",
        " pos_tags = nltk.pos_tag(words)\n",
        " print(pos_tags)"
      ],
      "metadata": {
        "id": "oo_RWVPeJD3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\tImplement the extraction / abstraction-based Text summarization on the paragraph of your own. (Paragraph must have at least 10 lines) "
      ],
      "metadata": {
        "id": "Qlr1-Jx3LBlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text summarization\n",
        "# !pip install sumy\n",
        "import nltk \n",
        "nltk.download('punkt') \n",
        "from sumy.parsers.plaintext import PlaintextParser \n",
        "from sumy.nlp.tokenizers import Tokenizer \n",
        "from sumy.summarizers.lsa import LsaSummarizer \n",
        "from sumy.utils import get_stop_words\n",
        "print('ENTER YOUR TEXT HERE : ') \n",
        "text = input()\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer = LsaSummarizer()\n",
        "summarizer.stop_words = get_stop_words(\"english\")\n",
        "summary = \" \".join([str(sentence) for sentence in summarizer(parser.document, 3)])\n",
        "print(\"TEXT SUMMARY:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "d5uyHhqOKInW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.\tImplement the program in python to convert the speech to text. Upon text conversion, tokenize the sentence and analyze the emotion for the same."
      ],
      "metadata": {
        "id": "QUXGFmmvLHL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install SpeechRecognition\n",
        "import speech_recognition as sr \n",
        "r = sr.Recognizer() \n",
        "file_path = \"/content/harvard.wav\"\n",
        "with sr.AudioFile(file_path) as source:\n",
        "  audio = r.record(source)\n",
        "try:\n",
        "  text = r.recognize_google(audio)\n",
        "  print(\"You said: \", text)\n",
        "\n",
        "except sr.UnknownValueError:\n",
        "  print(\"Sorry, I didn't understand that.\")\n"
      ],
      "metadata": {
        "id": "4yaEF53wLIl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\tWrite snippets to do the following using regular expression concept. The text can be created on your own; it must have 3 mail id’s, 2 phone numbers, \n",
        "-\tWrite RE to extract all Email id’s in the given text. \n",
        "-\tWrite RE to extract all mobile numbers.\n",
        "-\tWrite RE to extract the names from the below list which match a certain pattern S u _ _ _\n",
        "Sunil, Shyam, Ankit, Surjeet, Sumit, Subhi, Surbhi, Siddharth, Sujan\n",
        "-\tWork with the following functions:\n",
        "re.search(), re.match(), re.sub(), re.compile(), re.findall()\n",
        "-\tWrite RE matches a string that has 'ab' followed by zero or more 'c'.\n",
        "-\tWrite RE matches 'a' followed by zero or more copies of the sequence 'bc'\n",
        "-\tWrite RE matches 'ab' followed by zero or one 'c'\n"
      ],
      "metadata": {
        "id": "Kz9BWwl9WFzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import re\n",
        "text = \"My email is john@example.com.\"\n",
        "email_pattern = r'\\w+@\\w+\\.\\w+'\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "print(emails)\n",
        "\n",
        "#2\n",
        "import re\n",
        "text = \"My phone number is 123-456-7890. 9003363162\"\n",
        "phone_pattern = r'\\d{3}-?\\d{3}-?\\d{4}'\n",
        "phones = re.findall(phone_pattern, text)\n",
        "\n",
        "print(phones)\n",
        "\n",
        "#3\n",
        "import re\n",
        "names = ['Sunil', 'Shyam', 'Ankit', 'Surjeet', 'Sumit', 'Subhi', 'Surbhi', 'Siddharth', 'Sujan']\n",
        "name_pattern = r'Su\\w{3}'\n",
        "matching_names = [name for name in names if re.match(name_pattern, name)]\n",
        "\n",
        "print(matching_names)\n",
        "\n",
        "#4\n",
        "import re\n",
        "text = \"abcc abc abccc ab abbbc\"\n",
        "pattern = r'abc*'\n",
        "matches = re.findall(pattern, text)\n",
        "\n",
        "print(matches)\n",
        "\n",
        "#5\n",
        "import re\n",
        "text = \"a abcbc abcbcbc ab abcb\"\n",
        "pattern = r'ab?c*'\n",
        "matches = re.findall(pattern, text)\n",
        "\n",
        "print(matches)\n",
        "\n",
        "import re\n",
        "text = \"a abcbcbc abcbcbc ab abcb\"\n",
        "pattern = r'a?bc*'\n",
        "matches = re.findall(pattern, text)\n",
        "\n",
        "print(matches)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Je0zWLaxWJ7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}